
\section{Background}
\label{sec:background}

In this section, we review the dragonfly topology, including the placement and routing policies examined in previous work. 

\subsection{Dragonfly Network}
\label{sec:network}
The dragonfly is a two-level hierarchical topology, consisting of several groups connected by all-to-all links~\cite{kim-micro}. Each group consists of $a$ routers connected via all-to-all local channels. For each router, $p$ compute nodes are attached to it via terminal links, while $h$ links are used as global channels for intergroup connections. The resulting radix of each router is $k = a+h+p-1$. Different computing centers could choose different values for  $a$, $h$, $p$ when deploying their dragonfly network. The adoption of proper $a$, $h$, $p$ involves many factors such as system scale, building cost and workload characteristics. 

One prominent implementation of the dragonfly topology is the Cray Cascade system (Cray XC30)\cite{faanes}, which utilizes Aries routers~\cite{cray2012} as its building blocks. 
Cori is a Cray Cascade system which has been deployed at NERSC, Lawrence Berkeley National Laboratory. The building block of Cori is an Aries router with four terminal ports and 30 network ports. Each router has four compute nodes attached through terminal ports. Sixteen routers form a chassis and six chassis are put into the same group. Each router has 20 ports for local channels, that is 15 ports to connect all the other routers in the same chassis and 5 more ports to connect routers from other chassis. Each router has 10 ports for global channels for the connection between other groups. 


It is recommended that for load balancing purposes, a proper dragonfly configuration should follow $a=2p=2h$~\cite{kim-micro}. According to this configuration, the total number of groups, denoted as $g$ in the dragonfly network would be $g = a*h+1 $, the total number of compute nodes denoted as $N$ in the network would be $N = p*a*g $. In this work, we focus on the dragonfly topology that follow this configuration. 
An example dragonfly network is illustrated in Figure~\ref{fig:dragonfly-overview}. There are six routers in each group $(a=6)$, three compute nodes per router $(p=3)$, and three global channels per router $(h=3)$. This dragonfly network consists of 19 groups and 342 nodes in total.

\begin{figure}[h!] 
  \centering
  \includegraphics[width=0.48\textwidth]{dragonfly-overview}
  \caption{Five group slice of a 19-group dragonfly network. Job $J1$ is allocated using random placement, while Job $J2$ is allocated using contiguous placement.}
  \label{fig:dragonfly-overview}
\end{figure}


\subsection{Routing on Dragonfly}
\label{sec:routing-schemes}

The routing policy refers to the strategy adopted to route  packets from the source router to the destination router. Previously studied routing policies for dragonfly networks include minimal routing, adaptive routing~\cite{dally-dragonfly}, progressive adaptive routing~\cite{jiang} and variations thereof~\cite{won-prog-adaptive}. In this work we study three alternative routing policies considered by the community for dragonfly networks.

\textbf{Minimal:} In this policy, a packet takes the minimal (shortest) path from the source to the destination. The packet first routes locally from the source node to the global channel leading to the destination group. It traverses the global channel to the destination group and routes locally to the destination node. 
Minimal routing can guarantee the minimum hops a packet takes from the source to the destination. However, it usually results in congestion along the minimal paths. 

\textbf{Adaptive:} In this policy, the path a packet takes will be adaptively chosen between minimal and non-minimal paths, depending on the congestion situation along those paths. For the non-minimal path, an intermediate router in a separate group will be randomly chosen. The packet is forwarded to the intermediate router, connecting the source and destination groups through two separate minimal paths. Adaptive routing can avoid hot-spots in the presence of congestion and collapses to minimal routing otherwise. 
%\NOTE{The Universal Globally-Adaptive Load-balanced routing (Adaptive in our paper) chooses between minimal and non-minimal on a packet by packet basis depending on the state of the network to minimize the delay of the packet~\cite{jiang}.}

\textbf{Progressive Adaptive:} As opposed to adaptive routing, the decision to adaptively route a packet is continually re-evaluated within the source group until a non-minimal route is chosen; the re-evaluation does not occur in intermediate groups~\cite{jiang}.
Progressive adaptive routing is capable of handling scenarios where the minimal route is congested but the source router has not been informed yet.




\subsection{Job Placement on Dragonfly}
\label{sec:placement-schemes}

For a parallel application requiring multiple compute nodes, the job placement policy refers to the way of assigning the required number of nodes to the application by system software such as the batch scheduler~\cite{tsafrir-tpds-2007}. In this work, we study two alternative placement policies considered by the community for dragonfly systems: 

\textbf{Random Placement:} In this policy, an application gets the required number of nodes randomly from the available nodes in the system. As illustrated in Figure~\ref{fig:dragonfly-overview}, $J1$ is randomly allocated nodes attached to different routers in different groups. Routers may be shared by different applications and more routers are involved in serving each application when random placement is in use. Random placement can distribute the tasks of an application uniformly across the network to avoid the possible local congestion. 



\textbf{Contiguous Placement:} In this policy, the compute nodes are assigned to an application consecutively. The assignment first fills up a group, then crosses group boundaries as necessary. As illustrated in Figure~\ref{fig:dragonfly-overview}, $J2$ is allocated eight nodes by contiguous placement. Contiguous placement confines the tasks of an application into the same group and uses the minimum number of routers to serve each application, which may result in local network congestion and increase the possibility of hot-spots. 
