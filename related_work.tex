\section{Related Work}
\label{sec:related work}

The impact of job placement on system behavior and application performance has been the subject of many studies.
%~\cite{dskinner,abhinav-sc13,jose-ipdps15}. 
We focus on the HPC-centric studies here. Skinner et al.\ identified significant performance variability due to network contention~\cite{dskinner}. They found that performance variability is inevitable on either torus or fat-tree networks when network sharing among concurrently running applications is allowed.
%\TODO{what kind of system?}. 
Bhatele et al.\ studied the performance variability of a specific application, p3FD, running on different HPC production systems with torus network topologies \cite{abhinav-sc13}. They obtained performance consistency in their application when network resources were allocated compactly and exclusively and wide variability otherwise. Jokanovic et al. studied the impact of job placement to the workload and claimed that the key to reduce performance variability is to avoid network sharing~\cite{jose-ipdps15}. 

Recently, several researchers have investigated job placement and routing algorithms on dragonfly networks. Prisacari et al.\ proposed a communication-matrix-based analytic modeling framework for mapping application workloads onto network topologies~\cite{hoefler-hpdc14}. They found that, in the context of dragonfly networks, optimizing for throughput and not workload completion time is often misleading and the notion of system balance cited as a dragonfly design parameter is not always directly applicable to all workloads.
%\TODO{very brief blurb of what they did or what they found}. 
Jain et al. conducted a comprehensive analysis of various job placement and routing policies with regard to network link throughput on dragonfly network~\cite{jain-sc14}. Their work is based on an analytical model and synthetic workload. Bhatele et al. used coarse-grain simulation to study the performance of synthetic workloads under the different task mapping and routing policies on two-level direct networks~\cite{bhatele-sc11}. Mubarak et al.\ focused on  the modeling of large-scale dragonfly networks with parallel event driven simulation. The dragonfly network model for million-node configurations presented in their work strongly scales when going from 1,024 to 65,536 MPI tasks on IBM Blue Gene/P and IBM Blue Gene/Q systems~\cite{codes-dragonfly}. The dragonfly model used in this paper is from their work. 
%\TODO{mention Misbah's dragonfly work, for which our work is based on}.

Our work complements the literature in the following aspects. First, our simulations are driven by real application traces intended to be representative of production-scale application patterns. Second, we holistically examine network behavior at both the overall system level as well as the individual application level, though we do not consider communication-pattern specific application mappings as Prisacari et al.\ did. Third, with the CODES simulation toolkit and related network models~\cite{codes, codes-dragonfly},  
%\TODO{cite codes, Misbah's dragonfly work}, 
we are able to simulate and examine system and application behavior at a very fine grain, collecting data at the dragonfly link level with packet-level fidelity. We believe these differences allowed us to uncover the ``bully'' behavior, which to our knowledge is unreported in the literature. However, in a sense, Prisacari et al.'s work suggests these types of behaviors as possibilities deriving from the balance-first design rationale for the dragonfly.

