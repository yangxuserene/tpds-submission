

\section{Methodology}
\label{sec: methodology}

Configurable dragonfly networks that allow us to perform the exploration presented in this paper are hard to come by for the time being. Even with access to systems with such networks, job placement and routing policies are part of system configuration, which is impossible for users to make changes at will~\cite{jain-sc14, bhatele-sc11, zhou-ipdps-2015, jokanovic-ipdps-2015}. Therefore, we resort to simulation in our work.


\subsection{Simulation Tool}
\label{sec:simulation-tool}

We utilize the CODES simulation toolkit (Co-Design of Multilayer Exascale Storage Architectures)~\cite{misbah-tpds}, which builds upon the ROSS parallel discrete event simulator~\cite{carothers_ross:_2002,barnes_warp_2013} to enable exploratory study of large scale systems of interest to the HPC community. CODES supports dragonfly~\cite{codes-dragonfly, misbah-tpds}, torus~\cite{misbah-pads-2014, ning-pads-2011}, and Slim Fly~\cite{wolfe-pads-2016} networks with flit-level high-fidelity simulation. It can drive these models through an MPI simulation layer utilizing traces generated by the DUMPI MPI trace library available as part of the SST macro toolkit~\cite{sst}. 
The behavior and performance of the CODES dragonfly network model has been validated by Mubarak et al.\ \cite{codes-dragonfly} against BookSim, a serial cycle-accurate interconnection network simulator~\cite{booksim-simulator}.

\subsection{Parallel Applications}
\label{sec: application traces}



\begin{figure*}[th!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=1.9 in]{app_topology/amg}
        \caption{AMG}
        \label{fig:amg-communication-topology}
    \end{subfigure}%
%    \hspace{1em}%
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=1.9 in]{app_topology/mg}
        \caption{MultiGrid}
        \label{fig:mg-communication-topology}
    \end{subfigure}%
%    \hspace{1em}%
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[height=1.9 in]{app_topology/cr}
        \caption{CrystalRouter}
        \label{fig:cr-communication-topology}
    \end{subfigure}%
    \caption{
The communication matrix of each application. The label of both the x and the y axis is the index of MPI rank in each application. 
The legend bar on the right indicates the data transfer amount (Byte) between ranks in each application. The reddish background of each figure indicates that each application has some amount of all-to-all traffic.
}
   \label{fig:applications_communication_matrix}
\end{figure*}


We use a trace-driven approach to workload generation, choosing in particular three parallel application traces gathered to represent exascale workload behavior as part of the DOE Design Forward Project~\cite{designforward-webpage,designforward-traces}. Specifically, we study communication traces representing the \emph{Algebraic MultiGrid Solver} (AMG), \emph{Geometric Multigrid V-Cycle from Production Elliptic Solver} (MultiGrid) and \emph{Crystal Router MiniApp} (CrystalRouter). \footnote{The communication matrices of each application presented in Figures~\ref{fig:amg-communication-topology}, \ref{fig:cr-communication-topology}, and \ref{fig:mg-communication-topology}, respectively, are generated by the IPM data collected from the DOE Design Forward Project~\cite{designforward-traces}. }

\textbf{AMG:} The Algebraic MultiGrid Solver is a parallel algebraic multi-grid solver for linear systems arising from problems on unstructured mesh physics packages. It has been derived directly from the BoomerAMG solver developed in the Center for Applied Scientific Computing (CASC) at Lawrence Livermore National Laboratory~\cite{amg}. 

Figure~\ref{fig:amg-communication-topology} shows the communication matrix of AMG with 216 MPI ranks. Note that the dominant communication pattern of the application does not change with scale. We can observe that the dominant communication pattern of AMG is 3D nearest neighbor: each rank has intensive communication with up to six neighbors, with exception of those ranks on boundaries. The 3D nearest neighbor communication pattern is commonly seen in HPC parallel applications. PARTISN~\cite{partisn} and SNAP~\cite{snap} are two other applications with 3D nearest communication pattern. 

%\begin{figure}[h!] 
%  \centering
%  \includegraphics[width=0.5\textwidth]{app_topology/amg}
%  \caption{AMG communication matrix. The label of both the x and the y axis is the index of MPI rank in each application. The legend bar on the right indicates the data transfer amount (Byte) between ranks in each application. The reddish background of each figure indicates that each application has some amount of all-to-all traffic.}
%  \label{fig:amg-communication-topology}
%\end{figure}



\textbf{MultiGrid:} The geometric multi-grid v-cycle from the production elliptic solver BoxLib is a software framework for massively parallel block-structured adaptive mesh refinement (AMR) codes~\cite{boxlib}, which are used in structured grid physics packages. MultiGrid conforms to many-to-many communication pattern with decreasing message size and collectives for different parts of the multigrid v-cycle. 

Figure~\ref{fig:mg-communication-topology} shows the communication matrix of MultiGrid with 125 ranks. MultiGrid performs intensive communication between six neighbor ranks, similar to the 3D nearest-neighbor communication of AMG. However, MultiGrid's communication topology leads to a greater ``spread" of communication across the set of ranks, challenging the maximization of communication locality with respect to ranks. FillBoundary, another PDE solver code in~\cite{boxlib}, has similar dominant communication patterns as MultiGrid.  

%\begin{figure}[h!] 
%  \centering
%  \includegraphics[width=0.5\textwidth]{app_topology/mg}
%  \caption{MultiGrid communication matrix. The label of both the x and the y axis is the index of MPI rank in each application. The legend bar on the right indicates the data transfer amount (Byte) between ranks in each application. The reddish background of each figure indicates that each application has some amount of all-to-all traffic.}
%  \label{fig:mg-communication-topology}
%\end{figure}


\textbf{CrystalRouter:} The Crystal Router MiniApp is a communication kernel of Nek5000~\cite{nek5000}, a spectral element CFD application developed at Argonne National Laboratory. It features spectral element multi-grid solvers coupled with a highly scalable, parallel coarse-grid solver that is widely used for projects including ocean current modeling, thermal hydraulics of reactor cores, and spatiotemporal chaos. 

Crystal Router demonstrates the many-to-many communication pattern 
through a scalable multistage communication process. The collective communication in Crystal Router utilizes a recursive doubling approach. Ranks in Crystal Router conform to an $n$-dimensional hypercube and recursively split into ($n$-1)-dimensional hypercubes, with communication occurring along the splitting plane. The pattern of this communication is shown in Figure~\ref{fig:cr-communication-topology}. As a result of the logarithmic splitting process, a substantial portion of the communication occurs in small neighborhoods of ranks.

%\begin{figure}[h!] 
%  \centering
%  \includegraphics[width=0.5\textwidth]{app_topology/cr}
%  \caption{CrystalRouter communication matrix. The label of both the x and the y axis is the index of MPI rank in each application. The legend bar on the right indicates the data transfer amount (Byte) between ranks in each application. The reddish background of each figure indicates that each application has some amount of all-to-all traffic.}
%  \label{fig:cr-communication-topology}
%\end{figure}


\subsection{System Configuration}
\label{sec: simulation configuration}

The dragonfly network topology was originally envisioned by Kim et al.~\cite{kim-micro}. 
The parameters for building the dragonfly network studied in our work are chosen based on the model proposed in~\cite{kim-micro}. 
In our dragonfly network, each group consists of $a = 8$ routers connected via all-to-all local channels. For each router, there are $p = 4$ compute nodes attached to it via terminal links. Each router also has $h = 4$ global channels used for intergroup connections. The radix of each router is hence $k = a+h+p-1 = 15$. 
The total number of groups is $g = a*h+1 = 33 $ and the total number of compute nodes is $N = p*a*g = 1056$. 
%The dragonfly link bandwidths are 5.25 GiB/s for the local and terminal router-ports and 4.7 GiB/s for the global ports, inspired by the Cray Cascade system~\cite{faanes}.  
%\NOTE{What are these bandwidths? done!}. 
Being different from the Cray XC systems which have multiple global channels connecting a pair of groups~\cite{faanes}, dragonfly network simulated in this work uses single global channel between groups. For that particular reason, we use a higher bandwidth for the global channel and relative low bandwidth for the local and terminal channel.
The links in our dragonfly network are asymmetric, 2 Gib/s for the local and terminal router-ports and 4 GiB/s for the global ports.
In this work, we simulate the network performance and job interference across six different job placement and routing policy combinations, which are summarized in Table~\ref{tab: placement routing configs}.

%\footnote{With respect to random placement, we experiment with 50 sets of distinctive allocation generated by random placement. The corresponding experimental results are median chosen, which intended to eliminate the possibility of variation.}

\begin{table}[ht]
\begin{center}
\caption{Nomenclature for different placement and routing configurations} 
\label{tab: placement routing configs}
\begin{tabular}{l c c c }
\toprule % Top horizontal line
\toprule
&\multicolumn{3}{c}{Routing Policies} \\ 
\cmidrule(l){2-4}
Placement Policies & Minimal & Adaptive & Progressive Adaptive\\ % Column names row
\midrule % In-table horizontal line
%Contiguous  &  CM   &   CA   &  CPA   \\ % Content row 1
Contiguous  &  cont-min   &  cont-adp    &  cont-padp   \\ % Content row 1
\midrule
%Random  &   RM  &   RA   &  RPA   \\ 
Random  &   rand-min  &   rand-adp   &  rand-padp   \\ 
\midrule % In-table horizontal line
\bottomrule % Bottom horizontal line
\end{tabular}
\end{center}
\end{table}


We analyze both the overall network performance and the performance of each application.
Our analysis focuses on the following metrics:
\begin{itemize}

    \item \textbf{Network Traffic:} The traffic refers to the amount of data in bytes going through each router. We analyze the traffic on each terminal and on the local and global channels of each router. The network reaches optimal performance when the traffic is uniformly distributed and no particular network link is over-loaded. 
            
    \item \textbf{Network Saturation Time:} The saturation time refers to the time period when the buffer of a certain port in the router is full. We analyze the saturation time of ports corresponding to terminal links, local and global channels. The saturation time indicates the congestion level of routers. 
    
    \item \textbf{Communication Time:} The communication time of each MPI rank refers to the time it spends in completing all its message exchanges with other ranks. Due to the use of simulation, we are able to measure the absolute (simulated) time a message takes to reach its destination. The performance of each application is measured by the communication time distribution across all its ranks.    
    
    \end{itemize}

Note that we do not model computation for each MPI rank due to both the complexities inherent in performance prediction on separate parallel architectures as well as the emphasis on the side of the Design Forward traces on communication behavior rather than compute representation; users are instructed to treat the traces as if they came from one MPI rank-per-node configuration, despite being gathered using a rank-per-core approach. We follow the recommended interpretation in our simulations. 
%We instead allow MPI communications to proceed as quickly as they are able. 
%\TODO{is this a good enough explanation? Anything else we should add?}

\subsection{Workload Summary}
\label{sec:workload summary}

Two sets of parallel workloads are used in this study. Workload~\Rmnum{1} consists of AMG, MultiGrid and CrystalRouter. As shown in Table~\ref{tab:apps-detail}, AMG has the least amount of data transfer, making it the least communication-intensive job in the workload. 
CrystalRouter has the most amount of data transfer, which means it is the most communication-intensive job in Workload~\Rmnum{1}. 
Workload~\Rmnum{2} consists of sAMG, MultiGrid and CrystalRouter. sAMG, a synthetic version of AMG, is generated by increasing the data transferred in AMG's MPI calls by a factor of 100, making it the most communication-intensive job in the workload. We add sAMG for reasons that will become clear in the following sections.

As a significant portion of our experiments rely on nondeterministic behavior (random application allocation), we ran each configuration a total of 50 times with differing random seeds. We then chose a representative execution for presentation based on the median performance of each application. While there is variation in repeated runs of the following experiments, the resulting trends and observations are representative of the full suite of experimentation.

\begin{table}[ht]
\begin{center}
\caption{Summary of Applications}
\label{tab:apps-detail}
\begin{tabular}{l c c c }
\toprule % Top horizontal line
\toprule
App Name & Num. Rank & Avg. Data/Rank & Total Data\\ % Column names row
\midrule % In-table horizontal line
AMG  &    216 &   0.6MB   &     130MB\\ % Content row 1
\midrule
MultiGrid  &    125 &   5MB   &     625MB\\ 
\midrule
CrystalRouter  &   100  &  35MB    &     3500MB\\ 
\midrule
sAMG  &    216 &   60MB   &     13000MB\\ % Content row 1
\midrule % In-table horizontal line
\bottomrule % Bottom horizontal line
\end{tabular}
\end{center}
\end{table}

